\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,             % the size of the fonts that are used for the code
  breakatwhitespace=false,            % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 	   % sets automatic line breaking
  captionpos=b,                    	   % sets the caption-position to bottom
  commentstyle=\color{dkgreen},   % comment style
  deletekeywords={...},            	   % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},             % if you want to add LaTeX within your code
  extendedchars=true,                    % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                        % adds a frame around the code
  keepspaces=true,                         % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},          % keyword style
  otherkeywords={*,...},                % if you want to add more keywords to the set
  numbers=left,                               % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                           % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray},   % the style that is used for the line-numbers
  rulecolor=\color{black},                % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                       % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,              % underline spaces within strings only
  showtabs=false,                           % show tabs within strings adding particular underscores
  stepnumber=1,                             % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mauve},          % string literal style
  tabsize=2,	                                  % sets default tabsize to 2 spaces
  title=\lstname                               % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\chapter{Deeplearning4j (unfinished)}
{See also \cite{DL4J}\\
von http://deeplearning4j.org/quickstart\\
DL4J targets professional Java developers who are familiar with production deployments, IDEs and automated build tools.\\

How to:
http://deeplearning4j.org/documentation
...

%%%% SECTION %%%%
\section{Getting started (unfinished)}
von http://deeplearning4j.org/quickstart\\

\subsection{Vorraussetzungen und Empfehlungen (unfinished)}
- Java 1.7 oder höher (nur 64-Bit Version wird unterstützt\\
- Apache Maven (Maven is a dependency management and automated build tool for Java projects.) check https://books.sonatype.com/mvnex-book/reference/public-book.html for how to use\\
- IntelliJ IDEA oder Eclipse (An Integrated Development Environment (IDE) allows you to work with our API and configure neural networks in a few steps. We strongly recommend using IntelliJ, which communicates with Maven to handle dependencies.)\\
- Git

Installation:
http://deeplearning4j.org/gettingstarted
Follow the ND4J Getting Started instructions to start a new project and include necessary POM dependencies.
http://nd4j.org/getstarted.html
http://nd4j.org/dependencies.htm

- neues Projekt in IntelliJ im Anhang A


%%%% SECTION %%%%
\section{Feedforward Netze (unfinished)}
http://deeplearning4j.org/quickstart.html
Everything starts with a MultiLayerConfiguration, which organizes those layers and their hyperparameters.
Hyperparameters are variables that determine how a neural network learns. They include how many times to update the weights of the model, how to initialize those weights, which activation function to attach to the nodes, which optimization algorithm to use, and how fast the model should learn. This is what one configuration would look like:
\begin{lstlisting}
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
        .iterations(1)
        .weightInit(WeightInit.XAVIER)
        .activation("relu")
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .learningRate(0.05)
        // ... other hyperparameters
        .backprop(true)
        .build();
\end{lstlisting}
With Deeplearning4j, you add a layer by calling layer on the NeuralNetConfiguration.Builder(), specifying its place in the order of layers (the zero-indexed layer below is the input layer), the number of input and output nodes, nIn and nOut, as well as the type: DenseLayer.
\begin{lstlisting}
        .layer(0, new DenseLayer.Builder().nIn(784).nOut(250)
                .build())
\end{lstlisting}
Once you’ve configured your net, you train the model with model.fit.

Configuring the POM.xml File

To run DL4J in your own projects, we highly recommend using Maven for Java users, or a tool such as SBT for Scala. The basic set of dependencies and their versions are shown below. This includes:

   -  deeplearning4j-core, which contains the neural network implementations
   -  nd4j-native, the CPU version of the ND4J library that powers DL4J
    - canova-api - Canova is our library vectorizing and loading data

http://deeplearning4j.org/gettingstarted.html
Reproducible Results

Neural net weights are initialized randomly, which means the model begins learning from a different position in the weight space each time, which may lead it to different local optima. Users seeking reproducible results will need to use the same random weights, which they must initialize before the model is created. They can reinitialize with the same random weight with this line:

\begin{lstlisting}
   Nd4j.getRandom().setSeed(123);
\end{lstlisting}

%%%% SECTION %%%%
\section{Recurrent Neuronal Network (unfinished)}
\subsection{Builder}
http://deeplearning4j.org/doc/
Parametereinstellungen:
- iterations(int): Number of optimization iterations.
- learningRate(double): Learning rate. Defaults to 1e-1
- optimizationAlgo(OptimizationAlgorithm):  
- seed(long): Random number generator seed. Used for reproducability between runs
- biasInit(double): ??
- miniBatch(boolean): Process input as minibatch vs full dataset. Default set to true.
- updater(Updater): Gradient updater. 
- weightInit(WeightInit): Weight initialization scheme.

check http://deeplearning4j.org/glossary.html for explanations

- OptimizationAlgorithm: verf"ugbare Algorithmen %%CONJUGATE_GRADIENT, HESSIAN_FREE, LBFGS, LINE_GRADIENT_DESCENT, STOCHASTIC_GRADIENT_DESCENT
- Updater: For example, Updater.SGD for standard stochastic gradient descent, Updater.NESTEROV for Nesterov momentum, Updater.RSMPROP for RMSProp, etc. (alle verf"ugbaren: ADADELTA, ADAGRAD, ADAM, CUSTOM, NESTEROVS, NONE, RMSPROP, SGD)
- WeightInit:
DISTRIBUTION Distribution: Sample weights from a distribution based on shape of input
NORMALIZED Normalized: Normalize sample weights
RELU Delving Deep into Rectifiers
SIZE Size: Sample weights from bound uniform distribution using shape for min and max
UNIFORM Uniform: Sample weights from bound uniform distribution (specify min and max)
VI VI: Sample weights from variance normalized initialization (Glorot)
XAVIER N(0,2/nIn): He et al. (2015)
ZERO Zeros: Generate weights as zeros

\lstinputlisting[language=JAVA, firstline=1, lastline=10,  captionpos=b, caption={Beispiel zur RNN-Erstellung: Builder}]
{code_snippets/rnn_auszug1.java}

\subsection{ListBuilder}
first difference, for rnns we need to use GravesLSTM.Builder
we need to use RnnOutputLayer for our RNN

    // softmax normalizes the output neurons, the sum of all outputs is 1
    // this is required for our sampleFromDistribution-function

\lstinputlisting[language=JAVA, firstline=12, lastline=31,  captionpos=b, caption={Beispiel zur RNN-Erstellung: ListBuilder}]
{code_snippets/rnn_auszug1.java}

\subsection{Netz erzeugen}
\lstinputlisting[language=JAVA, firstline=33, lastline=37,  captionpos=b, caption={Beispiel zur RNN-Erstellung: Netz erzeugen}]
{code_snippets/rnn_auszug1.java}

\subsection{Daten erstellen}
// create input and output arrays: SAMPLE\_INDEX, INPUT\_NEURON,
    // SEQUENCE\_POSITION
\lstinputlisting[language=JAVA, firstline=41, lastline=55,  captionpos=b, caption={Beispiel zur RNN-Erstellung: Daten erstellen}]
{code_snippets/rnn_auszug1.java}

\subsection{Netz trainieren}
\lstinputlisting[language=JAVA, firstline=58, lastline=96,  captionpos=b, caption={Beispiel zur RNN-Erstellung:}]
{code_snippets/rnn_auszug1.java}

%%%% SECTION %%%%
\section{LSTM Netze (unfinished)}
A commented example of a Graves LSTM learning how to replicate Shakespearian drama, and implemented with Deeplearning4j
\subsubsection{Hyperparameter Tuning}
http://deeplearning4j.org/lstm.html
Here are a few ideas to keep in mind when manually optimizing hyperparameters for RNNs:
   - Watch out for overfitting, which happens when a neural network essentially "memorizes" the training data. Overfitting means you get great performance on training data, but the network’s model is useless for out-of-sample prediction.
   - Regularization helps: regularization methods include l1, l2, and dropout among others.
    -So have a separate test set on which the network doesn’t train.
   - The larger the network, the more powerful, but it’s also easier to overfit. Don’t want to try to learn a million parameters from 10,000 examples – parameters > examples = trouble.
  -  More data is almost always better, because it helps fight overfitting.
  -  Train over multiple epochs (complete passes through the dataset).
   - Evaluate test set performance at each epoch to know when to stop (early stopping).
  -  The learning rate is the single most important hyperparameter. Tune this using deeplearning4j-ui; see [this graph] %%(http://cs231n.github.io/neural-networks-3/#baby)
  -  In general, stacking layers can help.
  -  For LSTMs, use the softsign (not softmax) activation function over tanh (it’s faster and less prone to saturation (~0 gradients)).
  -  Updaters: RMSProp, AdaGrad or momentum (Nesterovs) are usually good choices. AdaGrad also decays the learning rate, which can help sometimes.
   - Finally, remember data normalization, MSE loss function + identity activation function for regression, Xavier weight initialization


} %% Ende Chapter{Deeplearning4j}