\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,             % the size of the fonts that are used for the code
  breakatwhitespace=false,            % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 	   % sets automatic line breaking
  captionpos=b,                    	   % sets the caption-position to bottom
  commentstyle=\color{dkgreen},   % comment style
  deletekeywords={...},            	   % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},             % if you want to add LaTeX within your code
  extendedchars=true,                    % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                        % adds a frame around the code
  keepspaces=true,                         % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},          % keyword style
  otherkeywords={*,...},                % if you want to add more keywords to the set
  numbers=left,                               % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                           % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray},   % the style that is used for the line-numbers
  rulecolor=\color{black},                % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                       % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,              % underline spaces within strings only
  showtabs=false,                           % show tabs within strings adding particular underscores
  stepnumber=1,                             % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mauve},          % string literal style
  tabsize=2,	                                  % sets default tabsize to 2 spaces
  title=\lstname                               % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\chapter{Deeplearning4J (unfinished)}
{See also \cite{DL4J}\\
von http://deeplearning4j.org/quickstart\\
DL4J targets professional Java developers who are familiar with production deployments, IDEs and automated build tools.\\

How to:
http://deeplearning4j.org/documentation
...

%%%% SECTION %%%%
\section{Getting started (unfinished)}
von http://deeplearning4j.org/quickstart\\

\subsection{Vorraussetzungen und Empfehlungen (unfinished)}
- Java 1.7 oder höher (nur 64-Bit Version wird unterstützt\\
- Apache Maven (Maven is a dependency management and automated build tool for Java projects.) check https://books.sonatype.com/mvnex-book/reference/public-book.html for how to use\\
- IntelliJ IDEA oder Eclipse (An Integrated Development Environment (IDE) allows you to work with our API and configure neural networks in a few steps. We strongly recommend using IntelliJ, which communicates with Maven to handle dependencies.)\\
- Git

Installation:
http://deeplearning4j.org/gettingstarted
Follow the ND4J Getting Started instructions to start a new project and include necessary POM dependencies.
http://nd4j.org/getstarted.html
http://nd4j.org/dependencies.htm

- neues Projekt in IntelliJ im Anhang A


%%%% SECTION %%%%
\section{Netzwerke erstellen (unfinished)}
Ein Neuronales Netz wird in DL4J durch drei Komponeten erstellt. Die Komponenten sind der NeuralNetConfiguration.Builder, der ListBuilder und die MultiLayerConfiguration. Nachfolgend werden zwei Beispiele gegeben wie die Implementation aussehen kann. Die 3-Schritt-Methode zeigt die Implementation jeder Komponente einzeln und die Kurzversion fasst die drei Schritte zusammen, was Codezeilen spart, aber f"ur Neulinge vermutlich etwas schwerer verst"andlich ist.\\
An zu merken ist noch, dass es sich bei dem gezeigten Code nicht um dasselbe Netzwerk handelt, sondern zwei verschiedene Netzwerke gezeigt werden. Da es hier nicht um einen Vergleich der beiden Methoden geht, sondern nur gezeigt werden soll in welcher Form eine Implementation m"oglich ist. Beide Codeausz"uge stammen von den Netzwerk-Beispielen, welche \cite{DL4J} zum Download zur Verf"ugung stellt.

\subsection{Ein Netzwerk erstellen (3-Schritt-Methode)}

\subsubsection{Schritt 1: NeuralNetConfiguration.Builder}
Mit Hilfe des NeuralNetConfiguration.Builder kann man die Netzparameter festlegen. Der Quellcode 3.1 enth"alt hierzu einen Auszug aus einem Beispielprogramm von \cite{DL4J}.
\lstinputlisting[language=JAVA, firstline=1, lastline=9,  captionpos=b, caption={NeuralNetConfiguration.Builder Beispiel}]
{code_snippets/rnn_auszug1.java}
Dieser Code enth"alt lediglich eine Auswahl aller einstellbaren Paramter, welche in der folgenden Tabelle \ref{tbl:beispieltabelle} zeilenweise erkl"art werden. F"ur Informationen zu weiteren verf"ugbaren Paramtern kann die DL4J Dokumentation zu Rate gezogen werden.

\begin{table}
\begin{tabular}{|p{0.8cm}|p{3.7cm}|p{8.8cm}|}\hline
   \textbf{Zeile} & \textbf{Parameter} & \textbf{Beschreibung} \\ \hline
   2 & iterations( int ) & Anzahl der Optimierungsdurchl"aufe \\ \hline
   3 & learningRate( double ) & Lernrate (Defaulteinstellung, wenn nichts anderes angegeben wurde, ist 1e-1) \\ \hline
   4 & optimizationAlgo( OptimizationAlgorithm ) & benutzter Optimierungsalgorithmus (zur Verf"ugung stehen: CONJUGATE\_GRADIENT, HESSIAN\_FREE, LBFGS, LINE\_GRADIENT\_DESCENT, STOCHASTIC\_GRADIENT\_DESCENT) \\ \hline
  5 & seed( long ) & Ursprungszahl f"ur Zufallszahlengenerator (wird zur Reproduzierbarkeit von Durchl"aufen benutzt) Neural net weights are initialized randomly, which means the model begins learning from a different position in the weight space each time, which may lead it to different local optima. Users seeking reproducible results will need to use the same random weights, which they must initialize before the model is created. \\ \hline
  6 & biasInit( double ) & Konstante zur Initialisierung der Netzwerk Bias (Default: 0.0) \\ \hline
  7 & miniBatch( boolean ) & Verarbeitet die Eingabe als Minibatch oder komplettes Datenset. (Default: true) \\ \hline
  8 & updater( Updater ) & Methode zum aktuallisieren des Gradienten (Updater.SGD for standard stochastic gradient descent, Updater.NESTEROV for Nesterov momentum, Updater.RSMPROP for RMSProp, etc. (alle verf"ugbaren: ADADELTA, ADAGRAD, ADAM, CUSTOM, NESTEROVS, NONE, RMSPROP, SGD)) \\ \hline
   9 & weightInit( WeightInit ) & Initiallationsschema der Gewichte
(zur Verf"ugung stehen:
DISTRIBUTION Distribution: Sample weights from a distribution based on shape of input
NORMALIZED Normalized: Normalize sample weights
RELU Delving Deep into Rectifiers
SIZE Size: Sample weights from bound uniform distribution using shape for min and max
UNIFORM Uniform: Sample weights from bound uniform distribution (specify min and max)
VI VI: Sample weights from variance normalized initialization (Glorot)
XAVIER N(0,2/nIn): He et al. (2015)
ZERO Zeros: Generate weights as zeros) \\ \hline
 \end{tabular}
\caption{"Ubersicht einiger Netzwerkparameter}
\label{tbl:beispieltabelle} % Verweis im Text mittels \ref{tbl:beispieltabelle}
\end{table}

Source:
http://deeplearning4j.org/doc/ \\
check http://deeplearning4j.org/glossary.html for explanations

\subsubsection{Schritt 2: ListBuilder}
Mit Hilfe des erstellten NeuralNetConfiguration.Builders kann ein ListBuilder erstellt werden (siehe Quellcode 3.2). Der ListBuilder ist f"ur die Netzstruktur zust"andig und verwaltet die Netzwerk-Layer. Beim Erstellen wird ihm die Anzahl aller verwendeten Layer mitgeteilt. (In diesem Beispiel wurde das Input Layer alse Hidden Layer mit gez"ahlt, wodurch bei der "Ubergabe der Layeranzahl lediglich das Output Layer hinzuaddiert werden muss.)

\lstinputlisting[language=JAVA, firstline=11, lastline=11,  captionpos=b, caption={Erstellen des ListBuilders}]
{code_snippets/rnn_auszug1.java}
Quellcode 3.3 zeigt das Erzeugen der einzelnen Layer f"ur ein RNN. RNNs nutzen in DL4J den GraveLSTM.Builder zum Erzeugen des Input und der Hidden Layer (siehe Zeile 2 bis 5). In Zeile 3 wird die Anzahl der Eingangsknoten "ubergeben, welche f"ur das Input Layer in diesem Beipiel die Anzahl aller zul"assigen Buchstaben ist und f"ur die Hidden Layer eine vorher festgelegt Konstante. Zeile 4 gibt die n"otigen Verbindungen zum folgenden Layer an, welche zwingend mit der Eingangsgr"o{\ss}e des n"achsten Layers "ubereinstimmen muss. Anschlie{\ss}end wird in Zeile 5 die Aktivierungsfunktion festgelegt, bevor in Zeile 6 des erstellte Layer dem ListBuilder "ubergeben wird.

Das Output Layer wird mit Hilfe des RnnOutputLayer.Builders erstellt (siehe Zeile 9 bis 12). Die "ubergebene LossFunction in Zeile 9 gibt die Methode an, mit der der Fehler zwischen Netzwerk-Ergebnis und tats"achlichem Ergebnis berechnet wird. Die Aktivierungsfunktion \glqq softmax\grqq{} in Zeile 10 normalisiert die Output Neuronen, so dass die Summe aller Ausgaben 1 ist. Zeile 12 gibt die Anzahl der Output Neuronen an, was in diesem Beispiel der Anzahl der zul"assigen Buchstaben entspricht.

\lstinputlisting[language=JAVA, firstline=13, lastline=25,  captionpos=b, caption={Erstellen der Netzwerk-Layer}]
{code_snippets/rnn_auszug1.java}

Anschlie{\ss}end kann der ListBuilder abgeschlossen werden (siehe Quellcode 3.4). Hierzu kann festgelegt werden, ob ein Vortrainieren stattfinden (Zeile 1) und/oder Backpropagation angewendet werden soll (Zeile 2).
\lstinputlisting[language=JAVA, firstline=28, lastline=30,  captionpos=b, caption={Fertigstellen des ListBuilder}]
{code_snippets/rnn_auszug1.java}

\subsubsection{Schritt 3: MultiLayerNetwork}
Wurde die Vorarbeit mit dem NeuralNetConfiguration.Builder und ListBuilder erledigt, kann wie im Quellcode 3.5 ein Netzwerk erstellt werden. Hierf"ur wird das MultiLayerNetwork verwendet, welches alle Informationen als MultiLayerConfigurations vom ListBuilder erh"allt. Nach der Initialisierung (Zeile 3) ist das Netzwerk bereit trainiert zu werden.
\lstinputlisting[language=JAVA, firstline=33, lastline=35,  captionpos=b, caption={Ein Netz erzeugen}]
{code_snippets/rnn_auszug1.java}

MultiLayerNetwork is a neural network with multiple layers in a stack, and usually an output layer. For neural networks with a more complex connection architecture, use ComputationGraph which allows for an arbitrary directed acyclic graph connection structure between layers. MultiLayerNetwork is trainable via backprop, with optional pretraining, depending on the type of layers it contains.
http://deeplearning4j.org/doc/

\subsection{Ein Netzwerk erstellen (Kurzversion)}
In diesem Beispiel wird ein Feedforward Netzwerk mit zwei Layern erstellt. Bei der Implementation des Quellcodes 3.6 wird auf die Unterteilung der Komponenten verzichtet und alle ben"otigten Netzwerkparameter sowie die Netzstruktur werden direkt in die MultiLayerConfiguration geschrieben ohne Variablen f"ur den NeuralNetConfiguration.Builder und ListBuilder anzulegen.
\lstinputlisting[language=JAVA, firstline=1, lastline=19,  captionpos=b, caption={Netzwerk erstellen Kurzversion Beispiel}]
{code_snippets/ffnn_auszug1.java}

\subsection{Layertypen}
DL4J stellt verschiedene Layerarten zum problemspezifischen Aufbau von Netzwerken zur Verf"ugung. So benutzt ein Beispiel von \cite{DL4J} sechs verschiedene Layertypen f"ur ein Netzwerk, welches jeden Frame eines Videos klassifiziert. 

F"ur ein simples Feedforward Netzwerk kann der DenseLayer.Builder() f"ur das Input und die Hidden Layer verwendet werden. Das Output Layer kann mittels OutputLayer.Builder() implementiert werden.

Zur Zeit stellt \cite{DL4J} kein spezielles Layer zur Implementierung von Recurrent Netzen zur Verf"gung und arbeiten in ihren Beispielen mit dem GraveLSTM.Builder() f"ur das Input und die Hidden Layer. F"ur das Output Layer steht der RnnOutputLayer.Builder() zur Verf"ugung.

LSTM Netzwerke werden ebenfalls mit dem GraveLSTM.Builder() und dem RnnOutputLayer.Builder() implementiert.


%%%% SECTION %%%%
\section{Netzwerke trainieren (unfinished)}
\subsection{Ein Netzwerk trainieren}
Once you’ve configured your net, you train the model with model.fit.
Ein erstelltes Netzwerk l"asst sich durch die Methode fit() trainieren. DataSet
\lstinputlisting[language=JAVA, firstline=61, lastline=61,  captionpos=b, caption={Ein Netz trainieren}]
{code_snippets/rnn_auszug1.java}
fit(org.nd4j.linalg.dataset.api.DataSet data)
Fit the model

\subsection{Daten erstellen (unfinished)}
// create input and output arrays: SAMPLE\_INDEX, INPUT\_NEURON,
    // SEQUENCE\_POSITION
\lstinputlisting[language=JAVA, firstline=40, lastline=54,  captionpos=b, caption={Beispiel zur RNN-Erstellung: Daten erstellen}]
{code_snippets/rnn_auszug1.java}


} %% Ende Chapter{Deeplearning4j}